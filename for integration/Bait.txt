python Script import os import sqlite3 import pandas as pd import numpy as np import tensorflow as tf from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Dense from tensorflow.keras.optimizers import Adam from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.metrics import mean_squared_error import threading import time import json import random import csv  import ThreadPoolExecutor from stable_baselines3 import PPO from stable_baselines3.common.env_util import make_vec_env from stable_baselines3.common.vec_env import DummyVecEnv from stable_baselines3.common.monitor import Monitor
Configuration
WEBSITE_URL = 'https://tnsr-q.io ' 
Database setup
DB_NAME = 'bot_logs.db' TABLE_BOT_VISITS = 'bot_visits' TABLE_BOT_WORK = 'bot_work'
Load bot interaction data from SQLite database
def load_data(): conn = sqlite3.connect(DB_NAME) visits_df = pd.read_sql_query(f'SELECT * FROM {TABLE_BOT_VISITS}', conn) work_df = pd.read_sql_query(f'SELECT * FROM {TABLE_BOT_WORK}', conn) conn.close() return visits_df, work_df
Preprocess the data
def preprocess_data(visits_df, work_df): # Extract features visits_df['timestamp'] = pd.to_datetime(visits_df['timestamp']) work_df['timestamp'] = pd.to_datetime(work_df['timestamp']) # Request frequency (requests per minute per IP) freq_df = visits_df.groupby(['bot_ip', pd.Grouper(key='timestamp', freq='1T')]).size().reset_index(name='request_count') freq_df = freq_df.groupby('bot_ip')['request_count'].mean().reset_index() freq_df.rename(columns={'request_count': 'avg_request_freq'}, inplace=True) # Task completion rate (valid results submitted) task_rate_df = work_df.groupby('bot_ip').size().reset_index(name='task_completion_rate') # Merge features merged_df = pd.merge(freq_df, task_rate_df, on='bot_ip', how='outer') merged_df.fillna(0, inplace=True) # Label bot behavior merged_df['behavior'] = merged_df.apply(lambda row: 'fast' if row['avg_request_freq'] > 10 else 'slow', axis=1) merged_df['is_worker'] = merged_df.apply(lambda row: 1 if row['task_completion_rate'] > 0 else 0, axis=1) return merged_df
Define a supervised learning model using TensorFlow
def create_supervised_model(): model = Sequential([ Dense(128, activation='relu', input_shape=(2,)), Dense(64, activation='relu'), Dense(32, activation='relu'), Dense(1, activation='linear') ]) model.compile(optimizer=Adam(learning_rate=0.001), loss='mse') return model
Train the supervised model
def train_supervised_model(model, X_train, y_train, X_test, y_test): model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test)) y_pred = model.predict(X_test) rmse = np.sqrt(mean_squared_error(y_test, y_pred)) print(f'Test RMSE: {rmse:.4f}') return model
Deploy the trained supervised model
def deploy_supervised_model(model): # Generate 5 new fake product prices fake_prices = [model.predict(np.array([[random.uniform(1, 10), random.uniform(1, 10)]]))[0][0] for _ in range(5)] # Update website with new products session = ()  product.body_html = f'A fake product created by bots with price {price:.2f}.' product.variants = [{'price': str(price)}] product.save() with ThreadPoolExecutor(max_workers=5) as executor: executor.map(create_product, fake_prices) # Create 5 SEO meta descriptions descriptions = [f'Discover our exclusive deal: {price:.2f} off on this product!' for price in fake_prices] # Save generated content to CSV with open('agent_output.csv', 'w', newline='') as csvfile: writer = csv.writer(csvfile) writer.writerow(['product_title', 'description']) for price in fake_prices: writer.writerow([f'Fake Product {price:.2f}', f'Discover our exclusive deal: ${price:.2f} off on this product!'])
Main function to run the pipeline
def main(): # Load data visits_df, work_df = load_data() # Preprocess data merged_df = preprocess_data(visits_df, work_df) # Split data into features and target X = merged_df[['avg_request_freq', 'task_completion_rate']] y = merged_df['result'] # Split data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Standardize features scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) # Create and train the supervised model model = create_supervised_model() model = train_supervised_model(model, X_train, y_train, X_test, y_test) # Save the trained model model.save('bot_agent_model.h5')